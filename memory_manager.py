"""
Zeniji Emotion Simul - Memory Manager
Ollama API ë° OpenRouter APIë¥¼ í†µí•œ LLM í˜¸ì¶œ ê´€ë¦¬
"""

import logging
import time
import requests
from typing import Optional, Tuple

import config

logger = logging.getLogger("MemoryManager")


class MemoryManager:
    """Ollama API ë° OpenRouter APIë¥¼ í†µí•œ LLM ê´€ë¦¬"""
    
    def __init__(self, dev_mode: bool = False, provider: str = None, model_name: str = None, api_key: str = None):
        self.provider = provider or config.LLM_PROVIDER
        self.dev_mode = dev_mode
        
        if self.provider == "openrouter":
            self.api_url = "https://openrouter.ai/api/v1"
            self.model_name = model_name or config.OPENROUTER_MODEL
            self.api_key = api_key or config.OPENROUTER_API_KEY
        else:  # ollama
            self.api_url = config.OLLAMA_API_URL
            self.model_name = model_name or config.OLLAMA_MODEL_NAME
            self.api_key = None
        
        self.is_loaded = False
    
    def load_model(self, force_reload: bool = False) -> Optional[Tuple[str, str]]:
        """
        LLM ëª¨ë¸ ë¡œë“œ í™•ì¸ (API ì—°ê²° í™•ì¸)
        Returns: (model_name, api_url) íŠœí”Œ (í˜¸í™˜ì„±ì„ ìœ„í•´)
        """
        if self.is_loaded and not force_reload:
            logger.info(f"{self.provider.upper()} model already loaded.")
            return self.model_name, self.api_url
        
        logger.info(f"[VRAM MANAGER] Checking {self.provider.upper()} API connection...")
        logger.info(f"[VRAM MANAGER] Provider: {self.provider}")
        logger.info(f"[VRAM MANAGER] API URL: {self.api_url}")
        logger.info(f"[VRAM MANAGER] Model: {self.model_name}")
        
        start = time.time()
        try:
            if self.provider == "openrouter":
                # OpenRouter API ì—°ê²° í™•ì¸
                if not self.api_key:
                    raise ValueError("OpenRouter API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­ìœ¼ë¡œ ì—°ê²° í™•ì¸
                test_payload = {
                    "model": self.model_name,
                    "messages": [{"role": "user", "content": "test"}],
                    "max_tokens": 1
                }
                
                response = requests.post(
                    f"{self.api_url}/chat/completions",
                    json=test_payload,
                    headers=headers,
                    timeout=10
                )
                
                if response.status_code == 401:
                    raise ValueError("OpenRouter API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                elif response.status_code != 200:
                    raise RuntimeError(f"OpenRouter API ì—°ê²° ì‹¤íŒ¨: HTTP {response.status_code}")
                
                logger.info(f"âœ… OpenRouter API ì—°ê²° í™•ì¸ ì™„ë£Œ")
                self.is_loaded = True
                duration = time.time() - start
                logger.info(f"[VRAM MANAGER] OpenRouter API ì—°ê²° í™•ì¸ ì™„ë£Œ. ({duration:.2f} s)")
                return self.model_name, self.api_url
                
            else:  # ollama
                # Ollama API ì—°ê²° í™•ì¸
                response = requests.get(f"{self.api_url}/api/tags", timeout=5)
                if response.status_code != 200:
                    raise RuntimeError(f"Ollama API ì—°ê²° ì‹¤íŒ¨: HTTP {response.status_code}")
                
                # ëª¨ë¸ ì¡´ì¬ í™•ì¸ (ì •í™•í•œ ì¼ì¹˜ ì‚¬ìš©)
                models = response.json().get("models", [])
                available_names = [m.get("name") for m in models]
                
                # ì •í™•í•œ ì¼ì¹˜ í™•ì¸ (íƒœê·¸ í¬í•¨í•œ ì „ì²´ ì´ë¦„ ë¹„êµ)
                model_exists = self.model_name in available_names
                
                if not model_exists:
                    logger.error(f"âŒ FATAL ERROR: ì„¤ì •ëœ ëª¨ë¸ '{self.model_name}'ì´ Ollamaì— ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    logger.error(f"ğŸ“‹ Ollamaì— í˜„ì¬ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ëª©ë¡:")
                    for name in available_names:
                        logger.error(f"   - {name}")
                    logger.error("")
                    logger.error("ğŸ”§ í•´ê²° ë°©ë²•:")
                    logger.error(f"   1. í„°ë¯¸ë„ì—ì„œ 'ollama list' ëª…ë ¹ìœ¼ë¡œ ì •í™•í•œ ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.")
                    logger.error(f"   2. config.pyì˜ OLLAMA_MODEL_NAMEì„ ì •í™•í•œ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.")
                    logger.error(f"   3. ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´: ollama pull {self.model_name}")
                    logger.error("")
                    logger.warning("âš ï¸  ëª¨ë¸ ì´ë¦„ì´ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ /api/generate í˜¸ì¶œ ì‹œ 404 ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤.")
                    # ê²½ê³ ë§Œ í•˜ê³  ê³„ì† ì§„í–‰ (ì‹¤ì œ ì˜¤ë¥˜ëŠ” /api/generateì—ì„œ ë°œìƒ)
                else:
                    logger.info(f"âœ… ëª¨ë¸ '{self.model_name}' í™•ì¸ë¨")
                
                self.is_loaded = True
                duration = time.time() - start
                logger.info(f"[VRAM MANAGER] Ollama API ì—°ê²° í™•ì¸ ì™„ë£Œ. ({duration:.2f} s)")
                
                if self.dev_mode:
                    self._log_dev_info(duration, available_names if not model_exists else None)
                
                return self.model_name, self.api_url
                
        except requests.exceptions.ConnectionError:
            if self.provider == "openrouter":
                error_msg = "OpenRouter APIì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”."
            else:
                error_msg = (
                    f"Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                    f"í™•ì¸ ì‚¬í•­:\n"
                    f"1. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸ (ollama serve)\n"
                    f"2. API URLì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸: {self.api_url}\n"
                    f"3. ë°©í™”ë²½ ì„¤ì • í™•ì¸"
                )
            logger.error(error_msg)
            return None
        except Exception as e:
            logger.error(f"Failed to connect to {self.provider.upper()}: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None
    
    def _log_dev_info(self, duration: float, available_models: Optional[list] = None):
        """Dev Mode: ìƒì„¸ ë¡œë“œ ì •ë³´ ì¶œë ¥"""
        logger.info("[DEV] Provider: %s", self.provider.upper())
        logger.info("[DEV] API URL: %s", self.api_url)
        logger.info("[DEV] Model: %s", self.model_name)
        logger.info("[DEV] Connection check time: %.2f s", duration)
        if available_models:
            logger.info("[DEV] Available models: %s", available_models)
        if self.provider == "ollama":
            logger.info("[DEV] Note: OllamaëŠ” ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ë˜ë©°, ëª¨ë¸ì€ Ollamaê°€ ê´€ë¦¬í•©ë‹ˆë‹¤.")
        elif self.provider == "openrouter":
            logger.info("[DEV] Note: OpenRouterëŠ” í´ë¼ìš°ë“œ ê¸°ë°˜ APIì…ë‹ˆë‹¤.")
    
    def generate(self, prompt: str, **kwargs) -> Optional[str]:
        """
        LLM APIë¥¼ í†µí•œ í…ìŠ¤íŠ¸ ìƒì„± (Ollama ë˜ëŠ” OpenRouter)
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„° (temperature, top_p, max_tokens ë“±)
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        if not self.is_loaded:
            logger.warning("Model not loaded. Attempting to load...")
            if self.load_model() is None:
                return None
        
        try:
            if self.provider == "openrouter":
                # OpenRouter API í˜¸ì¶œ
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": "https://github.com/zeniji/emotion-simul",
                    "X-Title": "Zeniji Emotion Simul"
                }
                
                payload = {
                    "model": self.model_name,
                    "messages": [
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": kwargs.get("temperature", config.LLM_CONFIG["temperature"]),
                    "top_p": kwargs.get("top_p", config.LLM_CONFIG["top_p"]),
                    "max_tokens": kwargs.get("max_tokens", config.LLM_CONFIG["max_tokens"]),
                }
                
                response = requests.post(
                    f"{self.api_url}/chat/completions",
                    json=payload,
                    headers=headers,
                    timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                )
                
                if response.status_code != 200:
                    logger.error(f"âŒ OpenRouter API í˜¸ì¶œ ì‹¤íŒ¨: HTTP {response.status_code}")
                    logger.error(f"Response: {response.text}")
                    return None
                
                result = response.json()
                generated_text = result.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
                
                if not generated_text:
                    logger.warning("OpenRouter returned empty response")
                    return None
                
                return generated_text
                
            else:  # ollama
                # Ollama API í˜¸ì¶œ
                payload = {
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": kwargs.get("temperature", config.LLM_CONFIG["temperature"]),
                        "top_p": kwargs.get("top_p", config.LLM_CONFIG["top_p"]),
                        "num_predict": kwargs.get("max_tokens", config.LLM_CONFIG["max_tokens"]),
                    }
                }
                
                response = requests.post(
                    f"{self.api_url}/api/generate",
                    json=payload,
                    timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                )
                
                if response.status_code != 200:
                    logger.error(f"âŒ Ollama API í˜¸ì¶œ ì‹¤íŒ¨: HTTP {response.status_code}")
                    logger.error(f"Response: {response.text}")
                    
                    # 404 ì˜¤ë¥˜ ì‹œ ëª¨ë¸ ì´ë¦„ ë¶ˆì¼ì¹˜ ê°€ëŠ¥ì„± ì•ˆë‚´
                    if response.status_code == 404:
                        logger.error("")
                        logger.error("ğŸ” ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°€ëŠ¥í•œ ì›ì¸:")
                        logger.error(f"   1. ëª¨ë¸ ì´ë¦„ ë¶ˆì¼ì¹˜: '{self.model_name}'ì´ Ollamaì— ì—†ìŠµë‹ˆë‹¤.")
                        logger.error("   2. 'ollama list' ëª…ë ¹ìœ¼ë¡œ ì •í™•í•œ ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.")
                        logger.error(f"   3. config.pyì˜ OLLAMA_MODEL_NAMEì„ ìˆ˜ì •í•˜ì„¸ìš”.")
                        logger.error("")
                    
                    return None
                
                result = response.json()
                generated_text = result.get("response", "").strip()
                
                if not generated_text:
                    logger.warning("Ollama returned empty response")
                    return None
                
                return generated_text
                
        except Exception as e:
            logger.error(f"{self.provider.upper()} generation failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None
    
    def offload_model(self):
        """OllamaëŠ” ë³„ë„ í”„ë¡œì„¸ìŠ¤ì´ë¯€ë¡œ ì–¸ë¡œë“œ ë¶ˆí•„ìš” (ë¡œê¹…ë§Œ)"""
        logger.info("[VRAM MANAGER] OllamaëŠ” ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ë˜ë¯€ë¡œ ì–¸ë¡œë“œê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.")
    
    def reload_model(self):
        """OllamaëŠ” ë³„ë„ í”„ë¡œì„¸ìŠ¤ì´ë¯€ë¡œ ì¬ë¡œë“œ ë¶ˆí•„ìš” (ë¡œê¹…ë§Œ)"""
        logger.info("[VRAM MANAGER] OllamaëŠ” ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ë˜ë¯€ë¡œ ì¬ë¡œë“œê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.")
    
    def unload_model(self):
        """OllamaëŠ” ë³„ë„ í”„ë¡œì„¸ìŠ¤ì´ë¯€ë¡œ ì–¸ë¡œë“œ ë¶ˆí•„ìš”"""
        self.is_loaded = False
        logger.info("Ollama connection marked as unloaded (Ollama ì„œë²„ëŠ” ê³„ì† ì‹¤í–‰ë©ë‹ˆë‹¤).")
    
    def get_model(self) -> Optional[Tuple[str, str]]:
        """í˜„ì¬ ì—°ê²°ëœ ëª¨ë¸ ì •ë³´ ë°˜í™˜ (ì—†ìœ¼ë©´ ì—°ê²° ì‹œë„)"""
        if not self.is_loaded:
            return self.load_model()
        return self.model_name, self.api_url
    
    def ensure_loaded(self) -> bool:
        """ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ë¡œë“œ"""
        if not self.is_loaded:
            return self.load_model() is not None
        return True
